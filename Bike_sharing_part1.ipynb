{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_data_importer(datafolder,start_year,num_years):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        datafolder: the filepath for the data, as a string\n",
    "        start_year: the initial year desired, in the format YYYY\n",
    "        num_years: the number of years desired, as a non-negative integer\n",
    "        \n",
    "    RETURN:\n",
    "        all_data: a pandas dataframe containing all the data\n",
    "    '''\n",
    "    # We initialise an empty dictionary.\n",
    "    all_data = {}\n",
    "    # Then we loop through the years...\n",
    "    for n in range(num_years):\n",
    "        # ...and through the quarters of each year.\n",
    "        for m in range(4):\n",
    "            # We assign a dictionary key of the form, e.g., 'Y2013Q2'.\n",
    "            # Then we give it its value: a pandas dataframe from the imported\n",
    "            #  csv data.\n",
    "            all_data['Y{0}'.format(str(int(n+start_year))+'Q'+str(int(m+1)))]=\\\n",
    "            pd.read_csv(datafolder+str(int(n+start_year))+r'Q'+str(int(m+1))+\\\n",
    "                        '-capitalbikeshare-tripdata.csv')\n",
    "    # Then we concatenate all the dataframes.\n",
    "    all_data = pd.concat(all_data.values(),axis=0).reset_index(drop=True)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = \"C:/Users/sneak/Documents/Python Scripts/Bike_sharing/\"\n",
    "start_year = 2013\n",
    "num_years = 5\n",
    "all_data = capital_data_importer(datafolder,start_year,num_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stations_name_num_splitter (all_data, datafolder=None):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        all_data  : a pandas dataframe containing the Capital bikesharing data\n",
    "        datafolder: (optional) a path to a folder, as a string\n",
    "    RETURN:\n",
    "        a dictionary:\n",
    "            'Station numbers': a pandas dataframe with columns 'Start date',\n",
    "                               'End date', 'Start stn', 'End stn'. Entries of\n",
    "                               'Start/End date' are strings, entries of\n",
    "                               'Start/End stn' are integers.\n",
    "            'Station names'  : a pandas dataframe with columns 'Start date',\n",
    "                               'End date', 'Start station', 'End station'.\n",
    "                               Entries of 'Start/End date' are strings, entries\n",
    "                               of 'Start/End station' are strings (names)\n",
    "\n",
    "    The function can also save station_names to csv in datafolder, so\n",
    "    station_names can be deleted from the RAM if desired.\n",
    "    \n",
    "    WARNING: calling this function with datafolder != None will overwrite\n",
    "    station_names.csv in datafolder!\n",
    "    '''\n",
    "\n",
    "\n",
    "    \n",
    "    if datafolder != None:\n",
    "        station_names =\\\n",
    "        all_data[['Start date','End date','Start station','End station']]\n",
    "        station_names.to_csv(datafolder+'station_names.csv',index=False)\n",
    "        del station_names\n",
    "\n",
    "    station_nums =\\\n",
    "    all_data.drop([\"Duration\",\"Start station\",\"End station\",\\\n",
    "                   \"Bike number\",\"Member type\"],axis=1)\n",
    "    station_nums.rename(columns={'Start station number':'Start stn',\\\n",
    "                                 'End station number':'End stn'},inplace=True)\n",
    "    station_nums['Start stn'] = station_nums['Start stn'].apply(int)\n",
    "    station_nums['End stn'] = station_nums['End stn'].apply(int)\n",
    "    # Now we find the minimum value:\n",
    "    stn_min = np.min(station_nums[\"Start stn\"])\n",
    "    # Then we subtract it from all the station labels:\n",
    "    station_nums[\"Start stn\"] = station_nums[\"Start stn\"] - stn_min\n",
    "    station_nums[\"End stn\"] = station_nums[\"End stn\"] - stn_min\n",
    "    \n",
    "    if datafolder == None:\n",
    "        output = {'Station numbers' : station_nums,\\\n",
    "                  'Station names' : station_names}\n",
    "    else:\n",
    "        output = {'Station numbers' : station_nums}\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data_dict = stations_name_num_splitter(all_data,datafolder=datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start date</th>\n",
       "      <th>End date</th>\n",
       "      <th>Start stn</th>\n",
       "      <th>End stn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01 00:03:55</td>\n",
       "      <td>2013-01-01 00:15:24</td>\n",
       "      <td>101</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-01 00:04:39</td>\n",
       "      <td>2013-01-01 00:16:19</td>\n",
       "      <td>236</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-01 00:10:01</td>\n",
       "      <td>2013-01-01 00:16:06</td>\n",
       "      <td>257</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-01 00:12:55</td>\n",
       "      <td>2013-01-01 00:17:47</td>\n",
       "      <td>614</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-01 00:13:24</td>\n",
       "      <td>2013-01-01 00:17:07</td>\n",
       "      <td>239</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Start date             End date  Start stn  End stn\n",
       "0  2013-01-01 00:03:55  2013-01-01 00:15:24        101      106\n",
       "1  2013-01-01 00:04:39  2013-01-01 00:16:19        236      304\n",
       "2  2013-01-01 00:10:01  2013-01-01 00:16:06        257      239\n",
       "3  2013-01-01 00:12:55  2013-01-01 00:17:47        614      612\n",
       "4  2013-01-01 00:13:24  2013-01-01 00:17:07        239      214"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data_dict[\"Station numbers\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "\n",
    "# filename = 'all_data.tex'\n",
    "# pdffile = 'all_data.pdf'\n",
    "# outname = 'all_data.png'\n",
    "\n",
    "# template = r'''\n",
    "# \\documentclass[preview]{{standalone}}\n",
    "# \\usepackage{{booktabs}}\n",
    "# \\begin{{document}}\n",
    "# {}\n",
    "# \\end{{document}}\n",
    "# '''\n",
    "\n",
    "# with open(filename, 'w') as f:\n",
    "#     f.write(template.format(display.to_latex()))\n",
    "\n",
    "# subprocess.call(['pdflatex', filename])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.call(['convert', '-density', '300', pdffile, '-quality', '90', outname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def station_binner(station_nums,time_interval='H'):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        station_nums : a dataframe with columns for dates and station numbers\n",
    "                       for the start and end of bike journeys; ideally 'Station\n",
    "                       numbers' from the output of stations_name_num_splitter\n",
    "        time_interval: (optional) a string specifying the time-intervals for\n",
    "                       making the bins; default value 'H' (1 hour)\n",
    "    RETURN:\n",
    "        a dictionary with 3 entries that are all dataframes:\n",
    "            'Starts by bin': the total journeys started from each station,\n",
    "                             arranged in columns by station number and in rows\n",
    "                             by the binning time_interval\n",
    "            'Ends by bin'  : the total journeys ended at each station, arranged\n",
    "                             in columns by station number and in rows by the\n",
    "                             binning time_interval\n",
    "            'Demand by bin': an approximation of the demand for bikes at each\n",
    "                             station, arranged in columns by station number and\n",
    "                             in rows by the binning time_interval\n",
    "    \n",
    "    N.B. The demand in each time_interval is calculated as follows: take the\n",
    "    total journeys started at each station in that time-interval, and subtract\n",
    "    the total journeys ended at the corresponding station in the previous\n",
    "    time-interval.\n",
    "    '''\n",
    "    \n",
    "    start_stns = pd.DataFrame(station_nums['Start stn'])\n",
    "    start_stns.index=pd.DatetimeIndex(station_nums['Start date'])\n",
    "    del start_stns.index.name\n",
    "    \n",
    "    end_stns = pd.DataFrame(station_nums['End stn'])\n",
    "    end_stns.index=pd.DatetimeIndex(station_nums['End date'])\n",
    "    del end_stns.index.name\n",
    "    \n",
    "    def resampled_vals_cntr(x):\n",
    "        y = pd.Series(x)\n",
    "        return y.value_counts().fillna(0)\n",
    "    \n",
    "    start_stns = start_stns.resample(time_interval)\n",
    "    list_starts = start_stns['Start stn'].apply(list)\n",
    "    starts_by_bin = list_starts.apply(lambda x: resampled_vals_cntr(x)).\\\n",
    "                                fillna(0).astype(int)\n",
    "\n",
    "    end_stns = end_stns.resample(time_interval)\n",
    "    list_ends = end_stns['End stn'].apply(list)\n",
    "    ends_by_bin = list_ends.apply(lambda x: resampled_vals_cntr(x)).fillna(0).\\\n",
    "                            astype(int)\n",
    "    ends_shifted = pd.concat([ends_by_bin.iloc[-1:],ends_by_bin.iloc[:-1]])\n",
    "    ends_shifted.index = ends_by_bin.index\n",
    "    \n",
    "    demand_by_bin = (starts_by_bin - ends_shifted).fillna(0).astype(int)\n",
    "    \n",
    "    return {'Starts by bin': starts_by_bin, 'Ends by bin' : ends_by_bin,\\\n",
    "            'Demand by bin' : demand_by_bin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_stations_dict = station_binner(split_data_dict['Station numbers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_by_hr = binned_stations_dict['Starts by bin']\n",
    "ends_by_hr = binned_stations_dict['Ends by bin']\n",
    "demand_by_hr = binned_stations_dict['Demand by bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.fft as npf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_fft(binned_data,figsize=None,title=None):\n",
    "    F_data = np.fft.fft(np.array(binned_data),axis=0)\n",
    "    T = (binned_data.index[1]-binned_data.index[0]).total_seconds()\n",
    "    ttl = title\n",
    "    \n",
    "    if isinstance(figsize, tuple):\n",
    "        fourier_plot(F_data,T,figsize=figsize,title=ttl)\n",
    "    \n",
    "    return F_data, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_plot(fourier_array,time_interval,figsize=(16,8),title=None):\n",
    "    N = np.size(fourier_array,axis=0)\n",
    "    F_X = np.linspace(0,1/(2*time_interval),N//(2))\n",
    "    plt.figure(figsize=figsize)\n",
    "    if isinstance(title, str):\n",
    "        plt.title(title)\n",
    "    arr_for_plot = fourier_array[:N//(2)] if fourier_array.ndim == 1 else fourier_array[:N//(2)]\n",
    "    plt.plot(F_X,np.abs(arr_for_plot/np.sqrt(N)))\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel('Normalised discrete Fourier transform (original units)')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_data, T = dataframe_fft(demand_by_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_plot(F_data,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use scipy's signal processing function, find_peaks, which finds the\n",
    "# positions of local maxima in 1-dim numpy arrays.\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# However find_peaks doesn't see peaks at end-points, so we make a small\n",
    "# adjustment to it.\n",
    "def find_peaks_ends (x, height):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        x and height from find_peaks\n",
    "        (https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find\n",
    "        _peaks.html)\n",
    "    RETURN:\n",
    "        same as find_peaks (positions of local maxima in x), except also finds\n",
    "        the end-points if they are local maxima\n",
    "    '''\n",
    "    \n",
    "    shift = 0\n",
    "    if (x[0] >= x[1]) & (x[-1] >= x[-2]):\n",
    "        y = np.append(np.insert(x, 0, x[0]-1), x[-1]-1)\n",
    "        shift = 1\n",
    "    elif x[0] >= x[1]:\n",
    "        y = np.insert(x, 0, x[0]-1)\n",
    "        shift = 1\n",
    "    elif x[-1] >= x[-2]:\n",
    "        y = np.append(x, x[-1]-1)\n",
    "    else:\n",
    "        y = x\n",
    "        \n",
    "    return find_peaks(y, height)[0] - shift\n",
    "    # We take the 0th component of the output to remove the \"height\"\n",
    "    # information automatically supplied by find_peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our second function takes find_peaks_ends and applies it to a specified axis\n",
    "# of a 2-dimensional numpy array.\n",
    "def max_ovr_thresh2 (array, T_interv, ax=0, x_thresh=0, y_thresh=0):\n",
    "    '''\n",
    "    INPUTS:\n",
    "        array   : a (non-empty) 2-dimensional numpy array\n",
    "        \n",
    "        T_interv: distance between two consecutive x-points\n",
    "        \n",
    "        ax      : the axis on which to find the maxima, i.e., 0 (maxima along\n",
    "                  each column) or 1 (maxima along each row)\n",
    "        x_thresh: a non-negative number, or a 1-dimensional numpy array of non-\n",
    "                  -negative numbers (length must equal the non-ax dimension of\n",
    "                  array!)\n",
    "        y_thresh: a non-negative number, or a 1-dimensional numpy array of non-\n",
    "                  -negative numbers (length must equal the non-ax dimension of\n",
    "                  array!)\n",
    "    \n",
    "    RETURN:\n",
    "        a dictionary of length the OTHER axis of array, i.e., 1 or 0 correspon-\n",
    "        -dingly, whose entries are 1-dim numpy arrays of the local maxima along\n",
    "        columns (ax=0) or rows (ax=1) that also exceed thresh\n",
    "    '''\n",
    " \n",
    "    if ax == 1:\n",
    "        array = np.transpose(array)\n",
    "    N = np.size(array,axis=0)\n",
    "    F_X_full = np.linspace(0,1/(T_interv),N)\n",
    "    \n",
    "    sz = np.size(array,axis=1)\n",
    "    if not np.shape(y_thresh): # checks if y_thresh is a scalar\n",
    "        y_thresh = [y_thresh for j in range(sz)] # then fills an array with the\n",
    "                                                 # scalar value\n",
    "    elif len(y_thresh) != sz: # otherwise checks if the vector y_thresh has an\n",
    "                              # appropriate size\n",
    "        raise ValueError(\"size of y_threshold ({0}) does not match size of \"+\\\n",
    "        \"desired axis of array ({1})\".format(y_thresh.size,sz))\n",
    "    \n",
    "    if not np.shape(x_thresh): # checks if x_thresh is a scalar\n",
    "        x_thresh = [x_thresh for j in range(sz)] # then fills an array with the\n",
    "                                                 # scalar value\n",
    "    elif len(x_thresh) != sz: # otherwise checks if the vector x_thresh has an\n",
    "                              # appropriate size\n",
    "        raise ValueError(\"size of x_threshold ({0}) does not match size of \"+\\\n",
    "        \"desired axis of array ({1})\".format(x_thresh.size,sz))\n",
    "\n",
    "    # We initialise our dictionary for the indices.\n",
    "    indices = {j : [] for j in range(sz)}\n",
    "    for j in range(np.size(array,axis=1)):\n",
    "    # Then for each column (row if ax==1) we find all the peaks above y_thresh.\n",
    "        indices[j] = find_peaks_ends(x = array[:,j], height = y_thresh[j])\n",
    "    # Then we choose those for which the x-value is less than x_thresh; since\n",
    "    # we're working with Fourier transform data this is symmetric so we also\n",
    "    # choose the x-values greater than the right-hand end minus x_thresh.\n",
    "        indices[j] = [idx for idx in indices[j] if\\\n",
    "                      F_X_full[idx] < x_thresh or\\\n",
    "                      F_X_full[idx] > F_X_full[-1] - x_thresh]\n",
    "        \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "def Fourier_series_from_index(f_array, indices):\n",
    "    F_dom_freqs = np.zeros(np.size(f_array))\n",
    "    for j in indices:\n",
    "        F_dom_freqs[indices[j],j] = f_array[indices[j],j]\n",
    "    reconstr = np.fft.ifft(F_dom_freqs,axis=0)\n",
    "    return reconstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mult_time_series\\\n",
    "(series_dict,start_date=None,end_date=None,figsize=(16,8)):\n",
    "    if start_date == None:\n",
    "        start_dts = pd.Series([series_dict[key].index[0]\\\n",
    "                               for key in series_dict])\n",
    "        start_date = start_dts.max()\n",
    "    if end_date == None:\n",
    "        end_dts = pd.Series([series_dict[key].index[-1]\\\n",
    "                             for key in series_dict])\n",
    "        end_date = end_dts.min()\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    for key in series_dict:\n",
    "        ax.plot(series_dict[key].loc[start_date:end_date],label=key)\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_demands = {'Demand by hour at station 7':demand_by_hr[7],'Demand by hour at station 15':demand_by_hr[15].iloc[24:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mult_time_series(some_demands,end_date='2013-01-31 23:00:00',figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekday_splitter(df,reset_index=False):\n",
    "    if not isinstance(df.index,pd.DatetimeIndex):\n",
    "        raise ValueError('Index must be of type pd.DatetimeIndex')\n",
    "    else:\n",
    "        df = df.copy() # to avoid over-writing errors\n",
    "        df['Day of the week'] = df.index.weekday\n",
    "        df['Workday'] = df['Day of the week']\\\n",
    "                            .apply(lambda x: 1 if x<5 else 0)\n",
    "        df['Hour and date'] = df.index\n",
    "        df_workdays = df[df.Workday == 1]\n",
    "        df_weekends = df[df.Workday == 0]\n",
    "        df_workdays = df_workdays.drop(['Day of the week','Workday'],axis=1)\n",
    "        df_weekends = df_weekends.drop(['Day of the week','Workday'],axis=1)\n",
    "###############################################################################\n",
    "        if reset_index:\n",
    "            time_interval = df.index[1]-df.index[0]\n",
    "            df_workdays.index\\\n",
    "            = pd.timedelta_range(start='0 days 0 hours',\\\n",
    "                                 periods=len(df_workdays),\\\n",
    "                                 freq = time_interval)\n",
    "            df_weekends.index\\\n",
    "            = pd.timedelta_range(start='0 days 0 hours',\\\n",
    "                                 periods=len(df_weekends),\n",
    "                                 freq = time_interval)\n",
    "    return {'Workdays' : df_workdays, 'Weekends' : df_weekends}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempor = demand_by_hr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = weekday_splitter(tempor,reset_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdays, weekends = split['Workdays'], split['Weekends']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrk_fourier, _ = dataframe_fft(workdays.drop([\"Hour and date\"],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_fourier, _ = dataframe_fft(weekends.drop([\"Hour and date\"],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_plot(wrk_fourier[:,9],T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrk_fourier.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 25\n",
    "pd.timedelta_range(start='0 days 00:00:00',end=t*(n+1),periods=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = demand_by_hr.index[1]-demand_by_hr.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
